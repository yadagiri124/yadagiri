This repository contains the implementation and experiments for Normalizing Flows (NF), specifically focusing on RealNVP and Glow models. Normalizing Flows provide a method for exact density estimation and generative modeling by using invertible transformations. The goal of this project is to evaluate how well these models learn data distributions and generate new samples, comparing their performance with traditional generative models such as GANs and VAEs.

Project Structure

train.py: The script for training RealNVP and Glow models on a synthetic dataset.

models.py: Contains the definitions and architectures for the RealNVP and Glow models.

utils.py: Utility functions for data preprocessing, visualizations, and model evaluation.

figures/: Folder where training curves, generated samples, and other visualizations are saved.

requirements.txt: Contains the list of Python libraries required to run the code.

README.md: This file.

Dataset

The dataset used in this project is synthetic 2D data, generated from a mixture of Gaussians. This dataset allows the models to learn the underlying distribution and generate new samples. The dataset consists of:

1,000 samples

2 features (x, y)

The data is split into a training set (80%) and a test set (20%).

The data is preprocessed by normalizing it to have zero mean and unit variance before training the models.

How to Run the Project

Install the dependencies:

Before running the project, ensure that all required libraries are installed. You can install the dependencies listed in requirements.txt by running:

pip install -r requirements.txt


Train the Models:

The models are trained on the synthetic 2D dataset. The training process is based on minimizing the Negative Log-Likelihood (NLL) loss, and the models are trained for 1000 epochs.

Visualize the Results:

Once the models are trained, you can visualize:

Training Curves: Negative log-likelihood (NLL) over epochs for both RealNVP and Glow models.

Generated Samples: Comparison of the original data distribution and the samples generated by both models.

These visualizations are saved in the figures/ directory and can be analyzed to compare the performance of the models.

Evaluation

The models are evaluated using the following metrics:

Negative Log-Likelihood (NLL): Measures the accuracy of the models in learning the true data distribution.

Generated Samples: Compares the samples generated by the models to the original data distribution to assess the quality of data generation.

Both RealNVP and Glow are trained and evaluated, with key performance differences based on their architectures. RealNVP shows faster convergence and better efficiency, while Glow excels in generating more diverse and high-quality samples due to its more flexible architecture, including invertible convolutions and multi-scale features.

Results

RealNVP: Converges faster and requires fewer coupling layers for training, making it more computationally efficient.

Glow: Generates more diverse samples and captures complex data patterns more effectively, but at the cost of increased training time.

The models show good performance on the density estimation task, with Glow offering better sample quality and diversity.

License

This project is licensed under the MIT License. See the LICENSE file for more details.
